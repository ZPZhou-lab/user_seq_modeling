import torch
from transformers import AutoModelForCausalLM, AutoConfig, PreTrainedModel
from .modeling import LlamaForCausalLM as CustomLlamaForCausalLM
from .modeling import Qwen3ForCausalLM as CustomQwen3ForCausalLM
from torch import nn
from torch.nn import functional as F
from dataclasses import asdict
from .arguments import ModelPath, TimeEmbeddingConfig
from .common import create_device_info, TimeStampEmbedding
from typing import Any


class UserEncoder(nn.Module):
    def __init__(self, 
        model_path: ModelPath, 
        ts_config: TimeEmbeddingConfig,
        use_flat_flash_attention: bool = True
    ):
        super(UserEncoder, self).__init__()
        self.local_rank, self.device = create_device_info()
        self.ts_config = ts_config

        # load pretrained llm
        hf_config = AutoConfig.from_pretrained(model_path.value, trust_remote_code=True)
        hf_config.use_cache = False
        hf_config.return_dict = True
        hf_config.use_ft_flash_attn = use_flat_flash_attention
        self.create_pretrained_model(model_path, hf_config)
        # add user token
        self.create_user_token()
        self.llm: PreTrainedModel = self.llm.base_model
        # remove the token_embedding layer
        self.llm.embed_tokens = None
        delattr(self.llm, 'embed_tokens')

        # create time embedding
        if self.ts_config.use_time_embedding:
            ts_kwargs = asdict(self.ts_config)
            ts_kwargs.pop('use_time_embedding')
            ts_kwargs['num_hiddens'] = hf_config.hidden_size
            self.embed_time = TimeStampEmbedding(**ts_kwargs).to(device=self.device, dtype=self.llm.dtype)

    def create_pretrained_model(self, model_path: ModelPath, hf_config: Any):
        """
        Create a pretrained model from the given model path.
        """
        print(f"Loading {model_path.name} User model...")
        if model_path.name.startswith("Qwen3"):
            self.llm = CustomQwen3ForCausalLM.from_pretrained(
                model_path.value,
                config=hf_config,
                torch_dtype=torch.bfloat16, device_map=self.device
            ).to(self.device)
        elif model_path.name.startswith("TinyLlama"):
            self.llm = CustomLlamaForCausalLM.from_pretrained(
                model_path.value,
                config=hf_config,
                torch_dtype=torch.bfloat16, device_map=self.device
            ).to(self.device)
    
    def forward(self, 
        event_embeddings: torch.Tensor, 
        attention_mask: torch.Tensor = None,
        user_varlen: torch.Tensor = None,
        user_position_ids: torch.Tensor = None,
        user_token_mask: torch.Tensor = None,
        time_ids: torch.Tensor = None,
        add_user_token: bool = True
    ):
        """
        encode user inputs into hidden_states

        Parameters
        ----------
        event_embeddings: (batch, seq_len, hiddens)
            The event embeddings generated by the EventEncoder.
        attention_mask: (batch, seq_len)
            The attention mask for the inputs to UserEncoder.
        user_varlen: (batch, )
            The variable length of the user sequence with shape (batch, )
        user_position_ids: (seq_len, )
            The position ids of the user sequence with shape (seq_len, )
        user_token_mask: (seq_len, )
            The mask for the user token in the user sequence with shape (seq_len, )
        time_ids: (batch, seq_len)
            The time ids for the inputs to UserEncoder. Only used if use_time_embedding is `True`.
        add_user_token: bool
            Whether to add user token to the input embeddings. Default is True.
        """
        device, dtype = self.device, event_embeddings.dtype
        # add time embedding
        if self.ts_config.use_time_embedding and time_ids is not None:
            event_embeddings += self.embed_time(time_ids) # (batch, seq_len, hiddens) or (seq_len, hiddens)

        # split two cases, user_varlen is None or not
        if user_varlen is None:
            if add_user_token:
                # user token can be padded to the end of the embeddings directly
                user_token = self.user_token.expand(event_embeddings.size(0), -1, -1).to(device, dtype=dtype)
                event_embeddings = torch.cat([event_embeddings, user_token], dim=1)
                attention_mask = torch.cat([
                    attention_mask, 
                    torch.ones((event_embeddings.size(0), 1), device=device, dtype=attention_mask.dtype)
                ], dim=1)

            # get embedding
            outputs = self.llm(
                inputs_embeds=event_embeddings,
                attention_mask=attention_mask,
                use_cache=False, output_hidden_states=False, output_attentions=False
            ).last_hidden_state
            
            # fetch the user token embedding
            if add_user_token:
                hidden_states, user_embedding = outputs[:, :-1, :], outputs[:, -1, :]
            else:
                hidden_states = outputs
                user_embedding = hidden_states.mean(dim=1) # mean pooling

            return hidden_states, user_embedding
        else:
            if add_user_token:
                # replace the last token of each user sequence with the user token
                bz = user_token_mask.sum().item()
                event_embeddings[user_token_mask] = self.user_token[0].expand(bz, -1).to(device, dtype=dtype)

            outputs = self.llm(
                inputs_embeds=event_embeddings.unsqueeze(0),
                position_ids=user_position_ids.unsqueeze(0),
                cu_input_lens=user_varlen,
                use_cache=False, output_hidden_states=False, output_attentions=False
            ).last_hidden_state.squeeze(0)

            if add_user_token:
                hidden_states = outputs[~user_token_mask] # (seq_len, hiddens)
                user_embedding = outputs[user_token_mask] # (batch, hiddens)
            else:
                hidden_states = outputs
                user_varlen_cum = F.pad(user_varlen.cumsum(dim=0), (1, 0), value=0)
                user_embedding = torch.stack([
                    outputs[s:e].mean(dim=0) for s, e in zip(user_varlen_cum[:-1], user_varlen_cum[1:])
                ]) # (batch, hiddens)

            return hidden_states, user_embedding


    def create_user_token(self, std=0.02, old_embeddings_weight=None):
        from torch.distributions import constraints
        # add user embedding token
        self.user_token = nn.Parameter(torch.zeros(1, 1, self.llm.config.hidden_size), requires_grad=True)
        
        # initialize the user token using the distribution of the pre-trained embedding
        embedding_dtype = self.llm.get_input_embeddings().weight.dtype
        old_embeddings_weight = self.llm.get_input_embeddings().weight.data.to(torch.float32)
        mean_embeddings = torch.mean(old_embeddings_weight, axis=0)
        old_centered_embeddings = old_embeddings_weight - mean_embeddings
        covariance = old_centered_embeddings.T @ old_centered_embeddings / old_embeddings_weight.size(0)

        # Check if the covariance is positive definite.
        epsilon = 1e-9
        is_covariance_psd = constraints.positive_definite.check(epsilon * covariance).all()
        if is_covariance_psd:
            # If covariances is positive definite, a distribution can be created. and we can sample new weights from it.
            distribution = torch.distributions.multivariate_normal.MultivariateNormal(
                mean_embeddings, covariance_matrix=epsilon * covariance
            )
            self.user_token.data = distribution.sample(sample_shape=(1,)).view(1, 1, -1).to(embedding_dtype)
        else:
            # Otherwise, just initialize with the mean. because distribution will not be created.
            self.user_token.data = mean_embeddings.view(1, 1, -1).to(embedding_dtype)


    def save_pretrained(self, save_path: str):
        """
        Save the pretrained model to the given path.
        """
        self.llm.save_pretrained(save_path)

    def from_pretrained(self, model_path: str):
        """
        Load the pretrained model from the given path.
        """
        self.llm = self.llm.from_pretrained(model_path)