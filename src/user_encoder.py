import torch
import os
from transformers import AutoModelForCausalLM, AutoConfig
from torch import nn
import numpy as np
import torch.nn.functional as F
import torch.distributed as dist
from .arguments import ModelPath


class UserEncoder(nn.Module):
    def __init__(self,
        model_path: ModelPath,
    ):
        super(UserEncoder, self).__init__()
        # create device
        if dist.is_initialized():
            local_rank = int(os.environ["LOCAL_RANK"])
            device = torch.device(f'cuda:{local_rank}')
        else:
            local_rank = 0
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        self.local_rank = local_rank
        self.device = device
        # load pretrained llm
        hf_config = AutoConfig.from_pretrained(model_path.value, trust_remote_code=True)
        hf_config.use_cache = False
        hf_config.return_dict = True
        print(f"Loading {model_path.name} User model...")
        llm = AutoModelForCausalLM.from_pretrained(
            model_path.value, 
            config=hf_config,
            torch_dtype=torch.bfloat16,
            device_map=self.device,
            trust_remote_code=True
        ).to(self.device)
        # remove embedding layer and only keep the encoder
        self.llm = llm.base_model
    
    def forward(self, 
        event_embeddings: torch.Tensor, 
        attention_mask: torch.Tensor
    ) -> torch.Tensor:
        """
        encode user inputs into hidden_states
        """
        # get embedding
        outputs = self.llm(
            inputs_embeds=event_embeddings,
            attention_mask=attention_mask.to(self.llm.device),
            use_cache=False,
            output_hidden_states=False,
            output_attentions=False
        )
        # extract the last hidden state with shape (batch, seq_len, hidden_size)
        return outputs.last_hidden_state
    

class GenerativeInfoNCELoss(nn.Module):
    def __init__(self, temperature=0.05, nce_threshold=0.99):
        super().__init__()
        self.temperature = nn.Parameter(torch.ones([]) * np.log(1 / temperature))
        self.nce_threshold = nce_threshold
        
    def forward(self, 
        predictions: torch.Tensor,
        positives: torch.Tensor,
        negatives: torch.Tensor,
        attention_mask: torch.Tensor
    ):
        """
        Parameters
        ----------
        predictions: (batch, seq_len, hidden)
            The outputs generated by the UserEncoder, are the next-event embeddings predictions.
        positives: (batch, seq_len, hidden)
            The positive event embeddings, generated by the EventEncoder, are the inputs of UserEncoder.
        negatives: (batch, num_neg, hidden)
            The sampled negative event embeddings, generated by the EventEncoder.
        attention_mask: (batch, seq_len)
            The attention mask for the inputs to UserEncoder.
        """
        batch_size, seq_len, hiddens = predictions.shape
        
        # postive samples
        predictions = predictions[:, :-1, :]  # (batch, seq_len-1, hidden)
        positives = positives[:, 1:, :]       # (batch, seq_len-1, hidden)
        
        with torch.no_grad():
            self.temperature.clamp_(0, np.log(100.0))
        temperature = self.temperature.exp()

        predictions = predictions / predictions.norm(dim=-1, keepdim=True)
        positives = positives / positives.norm(dim=-1, keepdim=True)
        megatives = negatives / negatives.norm(dim=-1, keepdim=True)

        # calculate the positive and negative scores
        # (batch, seq_len-1, 1)
        pos_scores = F.cosine_similarity(predictions, positives, dim=-1).unsqueeze(-1)

        # gather all negative samples from other devices
        if dist.is_initialized():
            negatives_all = all_gather(negatives, sync_grads=True) # (num_neg, hidden)
            negatives_all = negatives_all.reshape(-1, hiddens).transpose(-1, -2) # (hidden, num_neg)
        else:
            negatives_all = megatives.reshape(-1, hiddens).transpose(-1, -2) # (hidden, num_neg)
        
        neg_scores = torch.matmul(predictions, negatives_all) # (batch, seq_len-1, num_neg)
        # mask scores if the negative is similar to the positive
        mask = torch.matmul(positives, negatives_all) > self.nce_threshold
        neg_scores[mask] = torch.finfo(neg_scores.dtype).min

        # calculate the loss
        logits = torch.cat([pos_scores, neg_scores], dim=-1) # (batch, seq_len-1, num_neg + 1)
        logits = logits[attention_mask[:, :-1].bool()] * temperature # (batch * (seq_len - 1), num_neg + 1)
        labels = torch.zeros(logits.size(0), dtype=torch.long, device=logits.device)
        loss = F.cross_entropy(logits, labels)

        return loss


def all_gather(data,
               group=None,
               sync_grads=False):
    group = group if group is not None else torch.distributed.group.WORLD
    if torch.distributed.get_world_size() > 1:
        from torch.distributed import nn
        if sync_grads:
            return torch.stack(nn.functional.all_gather(data, group=group), dim=0)
        with torch.no_grad():
            return torch.stack(nn.functional.all_gather(data, group=group), dim=0)
    else:
        return data.unsqueeze(0)