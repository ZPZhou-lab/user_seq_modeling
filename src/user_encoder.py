import torch
import os
from transformers import AutoModelForCausalLM, AutoConfig, PreTrainedModel
from torch import nn
import numpy as np
import torch.nn.functional as F
import torch.distributed as dist
from dataclasses import asdict
from .arguments import ModelPath, TimeEmbeddingConfig
from .common import create_device_info, TimeStampEmbedding


class UserEncoder(nn.Module):
    def __init__(self, 
        model_path: ModelPath, 
        ts_config: TimeEmbeddingConfig
    ):
        super(UserEncoder, self).__init__()
        self.local_rank, self.device = create_device_info()
        self.ts_config = ts_config

        # load pretrained llm
        hf_config = AutoConfig.from_pretrained(model_path.value, trust_remote_code=True)
        hf_config.use_cache = False
        hf_config.return_dict = True
        print(f"Loading {model_path.name} User model...")
        llm = AutoModelForCausalLM.from_pretrained(
            model_path.value, 
            config=hf_config,
            torch_dtype=torch.bfloat16,
            device_map=self.device,
            trust_remote_code=True
        ).to(self.device)
        # remove embedding layer and only keep the encoder
        self.llm: PreTrainedModel = llm.base_model
        # add user token
        self.create_user_token()
        # remove the token_embedding layer
        self.llm.embed_tokens = None
        delattr(self.llm, 'embed_tokens')

        # create time embedding
        if self.ts_config.use_time_embedding:
            ts_kwargs = asdict(self.ts_config)
            ts_kwargs.pop('use_time_embedding')
            ts_kwargs['num_hiddens'] = hf_config.hidden_size
            self.embed_time = TimeStampEmbedding(**ts_kwargs).to(device=self.device, dtype=llm.dtype)

    
    def forward(self, 
        event_embeddings: torch.Tensor, 
        attention_mask: torch.Tensor,
        time_ids: torch.Tensor = None,
        add_user_token: bool = True
    ):
        """
        encode user inputs into hidden_states

        Parameters
        ----------
        event_embeddings: (batch, seq_len, hiddens)
            The event embeddings generated by the EventEncoder.
        attention_mask: (batch, seq_len)
            The attention mask for the inputs to UserEncoder.
        time_ids: (batch, seq_len)
            The time ids for the inputs to UserEncoder. Only used if use_time_embedding is `True`.
        add_user_token: bool
            Whether to add user token to the input embeddings. Default is True.
        """
        # add time embedding
        if self.ts_config.use_time_embedding and time_ids is not None:
            event_embeddings += self.embed_time(time_ids) # (batch, seq_len, hiddens)

        if add_user_token:
            # add user token to the input embeddings
            user_token = self.user_token.expand(event_embeddings.size(0), -1, -1).\
                to(self.device, dtype=event_embeddings.dtype)
            event_embeddings = torch.cat([event_embeddings, user_token], dim=1)
            attention_mask = torch.cat([
                attention_mask, 
                torch.ones((event_embeddings.size(0), 1), device=attention_mask.device, dtype=attention_mask.dtype)
            ], dim=1)
        
        # get embedding
        outputs = self.llm(
            inputs_embeds=event_embeddings,
            attention_mask=attention_mask.to(self.llm.device),
            use_cache=False,
            output_hidden_states=False,
            output_attentions=False
        ).last_hidden_state

        if add_user_token:
            hidden_states, user_embedding = outputs[:, :-1, :], outputs[:, -1, :]
        else:
            hidden_states = outputs
            user_embedding = hidden_states.mean(dim=1) # mean pooling

        return hidden_states, user_embedding
    

    def create_user_token(self, std=0.02, old_embeddings_weight=None):
        from torch.distributions import constraints
        # add user embedding token
        self.user_token = nn.Parameter(torch.zeros(1, 1, self.llm.config.hidden_size), requires_grad=True)
        
        # initialize the user token using the distribution of the pre-trained embedding
        embedding_dtype = self.llm.get_input_embeddings().weight.dtype
        old_embeddings_weight = self.llm.get_input_embeddings().weight.data.to(torch.float32)
        mean_embeddings = torch.mean(old_embeddings_weight, axis=0)
        old_centered_embeddings = old_embeddings_weight - mean_embeddings
        covariance = old_centered_embeddings.T @ old_centered_embeddings / old_embeddings_weight.size(0)

        # Check if the covariance is positive definite.
        epsilon = 1e-9
        is_covariance_psd = constraints.positive_definite.check(epsilon * covariance).all()
        if is_covariance_psd:
            # If covariances is positive definite, a distribution can be created. and we can sample new weights from it.
            distribution = torch.distributions.multivariate_normal.MultivariateNormal(
                mean_embeddings, covariance_matrix=epsilon * covariance
            )
            self.user_token.data = distribution.sample(sample_shape=(1,)).view(1, 1, -1).to(embedding_dtype)
        else:
            # Otherwise, just initialize with the mean. because distribution will not be created.
            self.user_token.data = mean_embeddings.view(1, 1, -1).to(embedding_dtype)


    def save_pretrained(self, save_path: str):
        """
        Save the pretrained model to the given path.
        """
        self.llm.save_pretrained(save_path)

    def from_pretrained(self, model_path: str):
        """
        Load the pretrained model from the given path.
        """
        self.llm = self.llm.from_pretrained(model_path)